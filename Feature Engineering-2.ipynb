{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cc885a-a01c-44ac-9fa6-b69c274b9aed",
   "metadata": {},
   "source": [
    "## Feature Engineering-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff14cf6-e581-41a2-9115-dd1375792b94",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9db87f-3182-497d-8704-0c99bc596bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dd9ae18-a130-4867-a890-834ae5baa7f9",
   "metadata": {},
   "source": [
    "#Q1 . what is the filter method infeachure selection and how does it work  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "74f55e66-b7c5-4d1d-bba1-a205d07e4511",
   "metadata": {},
   "source": [
    "Ans. The filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking. The advantage of using filter methods is that it needs low computational time and does not overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc767c-4bc2-46c1-be20-791c1572ea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79cec7fc-163e-49ad-b8e1-76cad5258c5b",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the filter method in feacher selection "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6acc58e-b992-4d52-829d-71a20e937875",
   "metadata": {},
   "source": [
    "The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af46151-a21c-426b-96ac-faffcd507874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5848d51f-0a9c-425f-a3f1-8cb1bed711dc",
   "metadata": {},
   "source": [
    "Q3. what aresome comman techniquesused in Embeddedfeachure selection methods "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef4c7ff8-8e2d-4f18-b8d8-7e1b094ca1f7",
   "metadata": {},
   "source": [
    "n an embedded method, feature selection is integrated or built into the classifier algorithm. During the training step, the classifier adjusts its internal parameters and determines the appropriate weights/importance given for each feature to produce the best classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93aa461-8f12-432c-b564-89ffe1f7bf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02fc20f1-e5d4-407c-a479-86ac3ecf361c",
   "metadata": {},
   "source": [
    "Q4. what are some drow back using the filter methods for feachure selection "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ecc0622-50a8-4739-97a8-af4ed122a698",
   "metadata": {},
   "source": [
    "The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold point for rankings to select only the required features and exclude noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945de96-8d00-46d9-a62d-165f56b5f15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ad726d-5318-4068-ac1b-fef1088adaa9",
   "metadata": {},
   "source": [
    "Q5. in which situationn would you prefer using the filter method over the wrapper method for feachure selection "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f60dd074-c2d6-4828-94b2-fafe2129db8b",
   "metadata": {},
   "source": [
    "Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abfee9-94b6-47f4-9553-177a8cfa15e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c70e21-85f1-4354-9424-becab9945941",
   "metadata": {},
   "source": [
    "Q5.In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b3256f7-3571-4c71-9183-382bab347bcb",
   "metadata": {},
   "source": [
    "For Telco companies it is key to attract new customers and at the same time avoid contract terminations (=churn) to grow their revenue generating base. Looking at churn, different reasons trigger customers to terminate their contracts, for example better price offers, more interesting packages, bad service experiences or change of customers’ personal situations.\n",
    "\n",
    "\n",
    "In this example, a basic machine learning pipeline based on a sample data set from Kaggle is build and performance of different model types is compared. The pipeline used for this example consists of 8 steps:\n",
    "\n",
    "Step 1: Problem Definition\n",
    "Step 2: Data Collection\n",
    "Step 3: Exploratory Data Analysis (EDA)\n",
    "Step 4: Feature Engineering\n",
    "Step 5: Train/Test Split\n",
    "Step 6: Model Evaluation Metrics Definition\n",
    "Step 7: Model Selection, Training, Prediction and Assessment\n",
    "Step 8: Hyperparameter Tuning/Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17074b5-5f33-4b55-8688-e7385cc7e933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3285c-2edb-4009-8501-930adabcb3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e987d-3a25-478d-bb02-bc6a76f7abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b6e66150-8751-4e98-9ccc-d074ce53ee7a",
   "metadata": {},
   "source": [
    "In the context of feature selection for a soccer match outcome prediction project, the Embedded method is a technique that combines feature selection with the model training process. It involves using algorithms that automatically select the most relevant features while fitting the model to the data. One popular algorithm that incorporates feature selection during training is LASSO (Least Absolute Shrinkage and Selection Operator).\n",
    "\n",
    "Here's a step-by-step explanation of how you can use the Embedded method to select the most relevant features for your soccer match outcome prediction model:\n",
    "\n",
    "1. Data Preprocessing: Start by preparing your dataset with various features related to player statistics, team rankings, and any other relevant information that could influence the outcome of a soccer match. Ensure that the data is cleaned, missing values are handled, and the features are appropriately encoded for modeling.\n",
    "\n",
    "2. Train-Test Split: Divide your dataset into a training set and a separate test set. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n",
    "\n",
    "3. Model Selection: Choose a machine learning algorithm suitable for soccer match outcome prediction. Common choices include logistic regression, support vector machines (SVM), random forests, or gradient boosting models. For this example, let's assume you choose logistic regression as your predictive model.\n",
    "\n",
    "4. Feature Selection with LASSO: LASSO is a linear regression model that adds a penalty term to the linear regression cost function. The penalty term is the L1 norm of the coefficients (weights) of the features. The L1 norm encourages some of the coefficients to become exactly zero, effectively performing feature selection. When features have zero coefficients, they are effectively excluded from the model.\n",
    "\n",
    "5. Regularization Strength (Hyperparameter): LASSO introduces a hyperparameter called \"alpha\" (λ) that controls the strength of the regularization penalty. Higher values of alpha lead to more aggressive feature selection, while lower values allow more features to be retained in the model. You can use techniques like cross-validation to determine the optimal value of alpha that works best for your dataset.\n",
    "\n",
    "6. Train the Model: Train your logistic regression model with LASSO regularization using the training dataset. The LASSO algorithm will automatically select the most relevant features during the training process.\n",
    "\n",
    "7. Evaluate the Model: Once the model is trained, use the test dataset to evaluate its performance. Compare the predicted outcomes with the actual outcomes to assess the model's accuracy, precision, recall, F1-score, or any other relevant metrics.\n",
    "\n",
    "8. Iterate and Refine: If necessary, repeat the process with different hyperparameter values or consider trying other embedded feature selection techniques, such as Ridge regression or Elastic Net, to further refine your feature selection process and improve the model's performance.\n",
    "\n",
    "By employing the Embedded method with LASSO, you can effectively select the most relevant features from your dataset and build a more interpretable and accurate soccer match outcome prediction model. Remember that the success of the method depends on the quality of the dataset, feature engineering, and choosing an appropriate model for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc5175-0d0c-422b-90b7-e62b963aa3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
